# PitVQA
## PitVQA Dataset
The dataset will be released upon acceptance of the paper
<div align='center'>
<img src='https://github.com/mobarakol/PitVQA/blob/main/assets/Dataset_Annaotation_Classes.png' width=550>
</div>

## Training Command:
For EndoVis18-VQA dataset:
```
python main_endo.py
```

For PitVQA dataset:
```
python main_pit.py
```
## Acknowledgement
The implementation of PitVQA relies on resources from <a href="https://github.com/salesforce/BLIP">BLIP</a>, <a href="https://github.com/huggingface/transformers">Huggingface Transformers</a>, and <a href="https://github.com/rwightman/pytorch-image-models/tree/master/timm">timm</a>. We thank the original authors for their open-sourcing.
